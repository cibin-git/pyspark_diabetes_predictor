

These are foundational models in ML and still heavily used in production systems.

â¸»

1ï¸âƒ£ Linear Regression

ğŸ“Œ What It Does

Predicts a continuous value.

Examples:
	â€¢	House price prediction
	â€¢	Sales forecasting
	â€¢	Temperature prediction

â¸»

ğŸ§® Model Equation

y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n

Or in vector form:

y = w^T x + b

Where:
	â€¢	x = features
	â€¢	w = weights
	â€¢	b = bias
	â€¢	y = predicted value

â¸»

ğŸ¯ Objective

Minimize Mean Squared Error (MSE):

MSE = \frac{1}{n} \sum (y_{true} - y_{pred})^2

â¸»

ğŸ Simple Python Example

from sklearn.linear_model import LinearRegression
import numpy as np

# Sample data
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])

model = LinearRegression()
model.fit(X, y)

print("Weight:", model.coef_)
print("Bias:", model.intercept_)
print("Prediction for 5:", model.predict([[5]]))

This learns:
y = 2x

â¸»

ğŸ“Š When to Use

âœ” Continuous output
âœ” Relationship roughly linear
âœ” Interpretability required

â¸»

2ï¸âƒ£ Logistic Regression

Despite the name, this is used for classification, not regression.

â¸»

ğŸ“Œ What It Does

Predicts probability of class membership.

Examples:
	â€¢	Spam vs Not Spam
	â€¢	Loan Default vs No Default
	â€¢	Fraud vs Genuine

â¸»

ğŸ§® Model Idea

First compute linear output:

z = w^T x + b

Then apply Sigmoid function:

\sigma(z) = \frac{1}{1 + e^{-z}}

This converts output into probability (0 to 1).

â¸»

ğŸ¯ Objective

Minimize Log Loss (Cross Entropy):

Loss = -[y \log(p) + (1-y)\log(1-p)]

â¸»

ğŸ Simple Python Example

from sklearn.linear_model import LogisticRegression
import numpy as np

# Sample data
X = np.array([[0], [1], [2], [3]])
y = np.array([0, 0, 1, 1])

model = LogisticRegression()
model.fit(X, y)

print("Prediction probability for 1.5:",
      model.predict_proba([[1.5]]))


â¸»

ğŸ“Š Key Differences

Linear Regression	Logistic Regression
Predicts continuous value	Predicts probability
Uses MSE loss	Uses Log loss
Output: (-âˆ, +âˆ)	Output: (0,1)
For regression tasks	For classification


â¸»

ğŸ” Geometric Interpretation

Both models:
	â€¢	Create a linear decision boundary
	â€¢	In 2D â†’ straight line
	â€¢	In 3D â†’ plane

Logistic regression separates classes using a hyperplane.

â¸»

ğŸ§  Why Linear Models Still Matter

Even in deep learning era:

âœ” Fast training
âœ” Highly interpretable
âœ” Works well on tabular data
âœ” Strong baseline model

For example:
	â€¢	Credit scoring systems
	â€¢	Risk modeling
	â€¢	Healthcare prediction systems

Many banking systems still rely on logistic regression because itâ€™s explainable.

â¸»

âš¡ Advanced Concepts (Optional)
	â€¢	L1 Regularization â†’ Lasso
	â€¢	L2 Regularization â†’ Ridge
	â€¢	Elastic Net â†’ Combination
	â€¢	Multiclass logistic regression (Softmax)

â¸»


	â€¢	Linear vs Polynomial regression
	â€¢	Regularization deeply
	â€¢	Gradient Descent math intuition
	â€¢	Logistic regression decision boundary visualization
	â€¢	How linear models connect to neural networks

