



---
# Unsupervised Learning — Practical Tutorial

This tutorial explains unsupervised learning, shows a typical workflow, and provides runnable Python examples using scikit-learn. The examples use synthetic datasets (make_blobs, make_moons, and Iris) so you can run them locally without external data.

## What is Unsupervised Learning?

Unsupervised learning discovers structure in unlabeled data. Common goals:
- Clustering: group similar samples (e.g., customer segmentation).
- Dimensionality reduction / embedding: compress or visualize high-dimensional data.
- Anomaly detection: find outliers or rare events.
- Topic modeling / latent factor discovery (text, recommender systems).

No target variable (y) is provided — the goal is exploration, representation, or preprocessing for downstream tasks.

## Typical Workflow

1. Problem framing: decide the goal (clusters, visualization, anomalies) and success criteria.  
2. Data exploration: inspect distributions, missingness, and feature types.  
3. Preprocessing: clean data, impute missing values, encode categoricals, and scale numeric features.  
4. Feature engineering: reduce noise, create aggregates, or compute embeddings for text/images.  
5. Algorithm selection: choose methods suited to data shape and goals (KMeans, DBSCAN, PCA, UMAP, etc.).  
6. Validation & evaluation: use internal metrics (silhouette, Davies–Bouldin) and qualitative checks (visualization, cluster profiling).  
7. Use or deploy: use cluster labels or embeddings as features, or deploy anomaly detectors with monitoring.

## Common Methods & When to Use Them

- KMeans: fast, scalable, works well for roughly spherical clusters in numeric data.  
- Gaussian Mixture Models (GMM): soft assignments and different covariance shapes.  
- DBSCAN / HDBSCAN: density-based clustering for arbitrary shapes and noise detection (no need to specify k).  
- PCA / TruncatedSVD: linear dimensionality reduction; fast and interpretable.  
- t-SNE / UMAP: nonlinear embeddings for visualization (UMAP is often faster and preserves more global structure).  
- IsolationForest / LocalOutlierFactor / OneClassSVM: anomaly detection methods.  
- Autoencoders / representation learning: learn nonlinear embeddings for downstream tasks.

## Evaluation & Validation

- Internal metrics: silhouette_score, Davies–Bouldin, Calinski–Harabasz.  
- External validation: if labels exist, use supervised metrics for validation.  
- Stability: run algorithms with multiple seeds and check cluster consistency.  
- Business validation: profile clusters and verify they are actionable/interpretable.

## Practical Examples (Python / scikit-learn)

Install requirements if needed:

pip install numpy matplotlib scikit-learn umap-learn

Example code snippets are ready to run in a Jupyter notebook or script.

### 1) Prepare sample datasets

```python
import numpy as np
from sklearn.datasets import load_iris, make_blobs, make_moons
from sklearn.preprocessing import StandardScaler

iris = load_iris()
X_iris = iris.data
y_iris = iris.target

X_blobs, _ = make_blobs(n_samples=500, centers=4, cluster_std=0.60, random_state=42)
X_moons, _ = make_moons(n_samples=500, noise=0.06, random_state=42)

scaler = StandardScaler()
X_iris_s = scaler.fit_transform(X_iris)
X_blobs_s = scaler.fit_transform(X_blobs)
X_moons_s = scaler.fit_transform(X_moons)
```

### 2) KMeans — elbow and silhouette to choose k

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

inertias = []
sil_scores = []
K = range(2, 9)
for k in K:
    km = KMeans(n_clusters=k, random_state=42).fit(X_blobs_s)
    inertias.append(km.inertia_)
    sil_scores.append(silhouette_score(X_blobs_s, km.labels_))

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(K, inertias, '-o')
plt.xlabel('k'); plt.ylabel('Inertia'); plt.title('Elbow method')

plt.subplot(1,2,2)
plt.plot(K, sil_scores, '-o')
plt.xlabel('k'); plt.ylabel('Silhouette score'); plt.title('Silhouette')
plt.show()

# Fit and visualize
km = KMeans(n_clusters=4, random_state=42).fit(X_blobs_s)
plt.scatter(X_blobs_s[:,0], X_blobs_s[:,1], c=km.labels_, cmap='tab10', s=20)
plt.title('KMeans clusters')
plt.show()
```

### 3) DBSCAN — arbitrary shapes and noise detection

```python
from sklearn.cluster import DBSCAN
db = DBSCAN(eps=0.2, min_samples=5).fit(X_moons_s)
labels_db = db.labels_  # -1 indicates noise/outliers

import matplotlib.pyplot as plt
plt.scatter(X_moons_s[:,0], X_moons_s[:,1], c=labels_db, cmap='tab10', s=20)
plt.title('DBSCAN results (moons)')
plt.show()
```

### 4) Dimensionality reduction — PCA and UMAP/t-SNE

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_iris_pca = pca.fit_transform(X_iris_s)

import matplotlib.pyplot as plt
plt.scatter(X_iris_pca[:,0], X_iris_pca[:,1], c=y_iris, cmap='tab10', s=30)
plt.title('Iris PCA (2D)')
plt.show()

# UMAP (falls back to TSNE if UMAP not installed)
try:
    import umap
    emb = umap.UMAP(n_components=2, random_state=42).fit_transform(X_blobs_s)
except Exception:
    from sklearn.manifold import TSNE
    emb = TSNE(n_components=2, random_state=42).fit_transform(X_blobs_s)

plt.scatter(emb[:,0], emb[:,1], c=km.labels_, cmap='tab10', s=20)
plt.title('UMAP/TSNE visualization of blobs (colored by KMeans)')
plt.show()
```

### 5) Anomaly detection — IsolationForest

```python
from sklearn.ensemble import IsolationForest
iso = IsolationForest(contamination=0.02, random_state=42)
iso.fit(X_blobs_s)
is_outlier = iso.predict(X_blobs_s) == -1

plt.scatter(X_blobs_s[:,0], X_blobs_s[:,1], c='lightgrey', s=20)
plt.scatter(X_blobs_s[is_outlier,0], X_blobs_s[is_outlier,1], c='red', s=30)
plt.title('IsolationForest outliers')
plt.show()
```

### 6) Pipelines and hyperparameter selection

Use sklearn Pipeline to avoid leakage. For clustering hyperparameters, use internal metrics like silhouette_score.

```python
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=5)),
    ('kmeans', KMeans(n_clusters=4, random_state=42))
])
pipe.fit(X_blobs)
labels_pipe = pipe.named_steps['kmeans'].labels_

# Choose k by silhouette
best = (None, -1)
for k in range(2,9):
    km = KMeans(n_clusters=k, random_state=42).fit(X_blobs_s)
    s = silhouette_score(X_blobs_s, km.labels_)
    if s > best[1]:
        best = (k, s)
print('Best k:', best)
```

## Validation Tips & Pitfalls

- Scale numeric features; many methods are distance-based.  
- One-hot encoding can explode dimensionality; consider domain-specific encodings or feature selection.  
- Use multiple internal metrics and visual checks; internal metrics are not a substitute for domain validation.  
- For high-dimensional noisy data, apply PCA/TruncatedSVD before clustering.

## Use Cases

- Customer segmentation  
- Anomaly detection (fraud, sensor monitoring)  
- Embedding + clustering for document/topic discovery  
- Dimensionality reduction for visualization and downstream features


