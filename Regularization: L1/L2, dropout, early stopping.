Great â€” now weâ€™re entering the core tools to control overfitting ğŸ”¥

Weâ€™ll cover:
	â€¢	âœ… L1 Regularization (Lasso)
	â€¢	âœ… L2 Regularization (Ridge)
	â€¢	âœ… Dropout (Neural Networks)
	â€¢	âœ… Early Stopping

â¸»

1ï¸âƒ£ L2 Regularization (Ridge)

ğŸ”¹ Idea

Add penalty on large weights.

Loss = MSE + \lambda \sum w^2
	â€¢	Penalizes large coefficients
	â€¢	Shrinks weights toward zero
	â€¢	Reduces variance
	â€¢	Keeps all features

ğŸ”¹ Effect
	â€¢	Smooths model
	â€¢	Prevents extreme curves
	â€¢	Good for multicollinearity

â¸»

ğŸ”¹ Simple Python Example (Ridge)

from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Polynomial + Ridge
ridge_model = make_pipeline(
    PolynomialFeatures(15),
    Ridge(alpha=1.0)  # lambda
)

ridge_model.fit(X_train, y_train)

print("Train MSE:", mean_squared_error(y_train, ridge_model.predict(X_train)))
print("Test  MSE:", mean_squared_error(y_test, ridge_model.predict(X_test)))

ğŸ‘‰ Increasing alpha = stronger regularization.

â¸»

2ï¸âƒ£ L1 Regularization (Lasso)

ğŸ”¹ Idea

Add penalty on absolute weights:

Loss = MSE + \lambda \sum |w|

ğŸ”¹ Key Difference from L2

L1	L2
Can make weights exactly 0	Shrinks weights
Feature selection	No feature elimination
Sparse model	Dense model


â¸»

ğŸ”¹ Python Example (Lasso)

from sklearn.linear_model import Lasso

lasso_model = make_pipeline(
    PolynomialFeatures(15),
    Lasso(alpha=0.01, max_iter=10000)
)

lasso_model.fit(X_train, y_train)

print("Train MSE:", mean_squared_error(y_train, lasso_model.predict(X_train)))
print("Test  MSE:", mean_squared_error(y_test, lasso_model.predict(X_test)))

ğŸ‘‰ Useful when you suspect many irrelevant features.

â¸»

3ï¸âƒ£ Dropout (Neural Networks)

Now we move to deep learning.

ğŸ”¹ Idea

Randomly â€œturn offâ€ neurons during training.

Example:
If dropout = 0.5 â†’ 50% neurons randomly deactivated per batch.

This:
	â€¢	Prevents co-dependency between neurons
	â€¢	Forces network to learn robust features
	â€¢	Reduces overfitting

â¸»

ğŸ”¹ Keras Example

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(64, activation='relu', input_shape=(10,)),
    Dropout(0.5),  # 50% dropout
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')


â¸»

4ï¸âƒ£ Early Stopping

ğŸ”¹ Idea

Stop training when validation loss starts increasing.

Typical pattern:
	â€¢	Training loss â†“ continuously
	â€¢	Validation loss â†“ then â†‘
	â€¢	Stop at minimum validation loss

â¸»

ğŸ”¹ Keras Example

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    callbacks=[early_stop]
)

ğŸ‘‰ Prevents overfitting without changing architecture.

â¸»

ğŸ”¥ When to Use What?

Technique	Works For	Reduces	Extra Benefit
L2 (Ridge)	Linear models, NN	Variance	Stable weights
L1 (Lasso)	High-dimensional data	Variance	Feature selection
Dropout	Neural networks	Variance	Implicit ensemble
Early stopping	Neural networks	Variance	Faster training


â¸»

ğŸ§  Intuition Summary
	â€¢	Overfitting = weights too large or model too flexible
	â€¢	Regularization = â€œpunish complexityâ€
	â€¢	Dropout = â€œrandom neuron removalâ€
	â€¢	Early stopping = â€œdonâ€™t train too longâ€

â¸»

If you want, next I can:
	â€¢	Show mathematical gradient intuition
	â€¢	Compare L1 vs L2 geometrically
	â€¢	Explain ElasticNet
	â€¢	Connect to biasâ€“variance mathematically
	â€¢	Or show how this applies in large Transformer models ğŸš€