{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.x"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Unsupervised Learning — Practical Notebook\n\nThis notebook contains runnable examples for clustering, dimensionality reduction, and anomaly detection using scikit-learn. It generates sample datasets and shows common workflows: scaling, pipelines, hyperparameter selection (internal metrics), and visualization."]},{"cell_type":"code","metadata":{},"source":["# Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris, make_moons, make_blobs\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.pipeline import Pipeline\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 1. Create sample datasets\nWe create three datasets: Iris (classic), blobs (for KMeans), and moons (for DBSCAN / non-spherical clusters)."]},{"cell_type":"code","metadata":{},"source":["# Iris dataset (for demonstration with labels, but we'll ignore labels for unsupervised tasks)\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\n# Blobs: well-separated clusters for KMeans\nX_blobs, y_blobs = make_blobs(n_samples=500, centers=4, cluster_std=0.60, random_state=42)\n\n# Moons: non-spherical clusters (good for DBSCAN/DBSCAN-like methods)\nX_moons, y_moons = make_moons(n_samples=500, noise=0.06, random_state=42)\n\n# Standardize datasets (important for distance-based methods)\nscaler = StandardScaler()\nX_iris_s = scaler.fit_transform(X_iris)\nX_blobs_s = scaler.fit_transform(X_blobs)\nX_moons_s = scaler.fit_transform(X_moons)\n\nprint('Shapes:', X_iris_s.shape, X_blobs_s.shape, X_moons_s.shape)"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 2. KMeans clustering — Elbow and Silhouette examples\nUse inertia (elbow) and silhouette score to select k."]},{"cell_type":"code","metadata":{},"source":["# Explore different k values on the blobs dataset\ninertias = []\nsil_scores = []\nK = range(2, 9)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42).fit(X_blobs_s)\n    inertias.append(km.inertia_)\n    sil_scores.append(silhouette_score(X_blobs_s, km.labels_))\n\nfig, ax = plt.subplots(1,2, figsize=(12,4))\nax[0].plot(K, inertias, '-o')\nax[0].set_xlabel('k')\nax[0].set_ylabel('Inertia')\nax[0].set_title('Elbow method (inertia)')\nax[1].plot(K, sil_scores, '-o')\nax[1].set_xlabel('k')\nax[1].set_ylabel('Silhouette score')\nax[1].set_title('Silhouette score')\nplt.show()\n\n# Fit KMeans with chosen k (e.g., 4) and visualize\nkm4 = KMeans(n_clusters=4, random_state=42).fit(X_blobs_s)\nplt.figure(figsize=(6,4))\nplt.scatter(X_blobs_s[:,0], X_blobs_s[:,1], c=km4.labels_, cmap='tab10', s=20)\nplt.title('KMeans (k=4) on blobs')\nplt.show()"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 3. DBSCAN — density-based clustering and outlier detection\nDBSCAN can find arbitrary-shaped clusters and produce a noise label (-1) for outliers."]},{"cell_type":"code","metadata":{},"source":["db = DBSCAN(eps=0.2, min_samples=5).fit(X_moons_s)\nlabels_db = db.labels_\nn_clusters = len(set(labels_db)) - (1 if -1 in labels_db else 0)\nn_noise = list(labels_db).count(-1)\nprint('Estimated clusters:', n_clusters, 'Noise points:', n_noise)\n\nplt.figure(figsize=(6,4))\nplt.scatter(X_moons_s[:,0], X_moons_s[:,1], c=labels_db, cmap='tab10', s=20)\nplt.title('DBSCAN on moons (labels: -1 = noise)')\nplt.show()"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 4. Dimensionality reduction — PCA and UMAP/TSNE for visualization\nPCA is linear and fast. UMAP or t-SNE are nonlinear and useful for 2D/3D visualizations. UMAP is recommended for speed and preserving global structure. This cell will try to use UMAP if available, otherwise fall back to TSNE."]},{"cell_type":"code","metadata":{},"source":["# PCA example on Iris (reduce to 2D for plotting)\npca = PCA(n_components=2)\nX_iris_pca = pca.fit_transform(X_iris_s)\nplt.figure(figsize=(6,4))\nplt.scatter(X_iris_pca[:,0], X_iris_pca[:,1], c=y_iris, cmap='tab10', s=30)\nplt.title('Iris PCA (2D)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()\n\n# Try UMAP, else use TSNE\ntry:\n    import umap\n    emb = umap.UMAP(n_components=2, random_state=42).fit_transform(X_blobs_s)\n    method = 'UMAP'\nexcept Exception as e:\n    from sklearn.manifold import TSNE\n    emb = TSNE(n_components=2, random_state=42).fit_transform(X_blobs_s)\n    method = 'TSNE'\n\nplt.figure(figsize=(6,4))\nplt.scatter(emb[:,0], emb[:,1], c=km4.labels_, cmap='tab10', s=20)\nplt.title(f'{method} visualization of blobs colored by KMeans labels')\nplt.show()"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 5. Anomaly detection — IsolationForest\nIsolationForest is an unsupervised method that detects outliers. We'll run it on the blobs dataset and visualize flagged points."]},{"cell_type":"code","metadata":{},"source":["iso = IsolationForest(contamination=0.02, random_state=42)\niso.fit(X_blobs_s)\nis_outlier = iso.predict(X_blobs_s) == -1\n\nplt.figure(figsize=(6,4))\nplt.scatter(X_blobs_s[:,0], X_blobs_s[:,1], c='lightgrey', s=20)\nplt.scatter(X_blobs_s[is_outlier,0], X_blobs_s[is_outlier,1], c='red', s=30, label='outliers')\nplt.title('IsolationForest outliers (on blobs)')\nplt.legend()\nplt.show()"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 6. Pipeline example: scaling → PCA → KMeans\nUse sklearn Pipeline to encapsulate preprocessing and model. This prevents leakage and makes experiments reproducible."]},{"cell_type":"code","metadata":{},"source":["pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=5)),\n    ('kmeans', KMeans(n_clusters=4, random_state=42))\n])\npipe.fit(X_blobs)\nlabels_pipe = pipe.named_steps['kmeans'].labels_\nprint('Labels from pipeline (unique):', np.unique(labels_pipe))\n\nplt.figure(figsize=(6,4))\nplt.scatter(X_blobs_s[:,0], X_blobs_s[:,1], c=labels_pipe, cmap='tab10', s=20)\nplt.title('Pipeline: scaler → PCA → KMeans')\nplt.show()"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 7. Hyperparameter selection (internal metric)\nFor clustering, use internal metrics like silhouette score to choose parameters. Here we pick k for KMeans by silhouette on the blobs dataset."]},{"cell_type":"code","metadata":{},"source":["best = (None, -1)  # (k, score)\nfor k in range(2,9):\n    km = KMeans(n_clusters=k, random_state=42).fit(X_blobs_s)\n    s = silhouette_score(X_blobs_s, km.labels_)\n    print('k=', k, 'silhouette=', round(s,4))\n    if s > best[1]:\n        best = (k, s)\nprint('Best k by silhouette:', best)"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 8. Next steps and notes\n- For production, save fitted transformers and models (joblib, pickle), and build an inference pipeline.\n- For large datasets, use minibatch KMeans or scalable libraries (Dask-ML, Faiss for nearest neighbors).\n- For text data, embed text (sentence-transformers) and then cluster embeddings.\n\nReferences:\n- scikit-learn: https://scikit-learn.org\n- UMAP: https://umap-learn.readthedocs.io\n- t-SNE in scikit-learn: sklearn.manifold.TSNE"]}]}